{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7VVH9nSt3bt"
      },
      "source": [
        "# Setting up the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8ilEHtht3bv"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Settings the warnings to be ignored\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUnQMNXaCiel"
      },
      "outputs": [],
      "source": [
        "# Uncomment when in Google Colab\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install py7zr\n",
        "!pip install scikit-learn\n",
        "!pip install Pillow\n",
        "!pip install pandas\n",
        "!pip install opencv-python\n",
        "!pip install albumentations\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2a23Q-xfPgY"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "import py7zr\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmPO1itMRHfi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzSXermBPNXi"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/TrainingDataset_Pan.zip\"\n",
        "# extract files from landfill_dataset.7z\n",
        "# in_path = './TrainingDataset_Pan.7z'\n",
        "# out_path = './'\n",
        "# with py7zr.SevenZipFile(in_path, mode='r') as z:\n",
        "#     z.extractall(out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxZdDRzLt3bx"
      },
      "source": [
        "# Create Image Segmentation dataset for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie3wHlDxfdM4"
      },
      "outputs": [],
      "source": [
        "class ImageSegmentationDataset(Dataset):\n",
        "    \"\"\"Image segmentation dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, feature_extractor, transforms=None, train=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
        "            feature_extractor (SegFormerFeatureExtractor): feature extractor to prepare images + segmentation maps.\n",
        "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.train = train\n",
        "        self.transforms = transforms\n",
        "\n",
        "        sub_path = \"train\" if self.train else \"validation\"\n",
        "        self.img_dir = os.path.join(self.root_dir, \"images\", sub_path)\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"mask\", sub_path)\n",
        "\n",
        "        # read images\n",
        "        image_file_names = []\n",
        "        for root, dirs, files in os.walk(self.img_dir):\n",
        "            image_file_names.extend(files)\n",
        "        self.images = sorted(image_file_names)\n",
        "\n",
        "        # read annotations\n",
        "        annotation_file_names = []\n",
        "        for root, dirs, files in os.walk(self.ann_dir):\n",
        "            annotation_file_names.extend(files)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = cv2.imread(os.path.join(self.img_dir, self.images[idx]))\n",
        "\n",
        "        segmentation_map = cv2.imread(os.path.join(self.ann_dir, self.annotations[idx]))\n",
        "        segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n",
        "        # Convert all 255 to 1 for metrics\n",
        "        segmentation_map[segmentation_map == 255] = 1\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            augmented = self.transforms(image=image, mask=segmentation_map)\n",
        "            encoded_inputs = self.feature_extractor(augmented['image'], augmented['mask'], return_tensors=\"pt\")\n",
        "        else:\n",
        "            encoded_inputs = self.feature_extractor(image, segmentation_map, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "            encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msH36qH_feF4"
      },
      "outputs": [],
      "source": [
        "WIDTH = 512\n",
        "HEIGHT = 512\n",
        "\n",
        "transform = A.Compose([\n",
        "        A.augmentations.Rotate(limit=90, p=0.5),\n",
        "        A.augmentations.HorizontalFlip(p=0.5),\n",
        "        A.augmentations.VerticalFlip(p=0.5),\n",
        "        A.augmentations.transforms.ColorJitter(p=0.5),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WdG6bFOffmP"
      },
      "outputs": [],
      "source": [
        "# Retrieve training set and validation set\n",
        "root_dir = './TrainingDataset_Pan' # remove /content/ when not in colab\n",
        "feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
        "\n",
        "train_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=transform)\n",
        "valid_dataset = ImageSegmentationDataset(root_dir=root_dir, feature_extractor=feature_extractor, transforms=None, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-srAEcshQtjt"
      },
      "outputs": [],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(valid_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1GA6V_Jg2cL"
      },
      "outputs": [],
      "source": [
        "encoded_inputs = train_dataset[0]\n",
        "print(\"encoded_inputs['pixel_values'] shape: \", encoded_inputs[\"pixel_values\"].shape)\n",
        "print(\"encoded_inputs['labels'] shape: \", encoded_inputs[\"labels\"].shape, \"unique:\", encoded_inputs[\"labels\"].squeeze().unique())\n",
        "print(\"encoded_inputs['labels']: \", encoded_inputs['labels'])\n",
        "encoded_inputs[\"labels\"].squeeze().unique()\n",
        "mask = encoded_inputs[\"labels\"].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv0X808WhjoH"
      },
      "outputs": [],
      "source": [
        "mask = encoded_inputs[\"labels\"].numpy()\n",
        "plt.imshow(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLBLtbDBQzYN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 1\n",
        "epochs = 31\n",
        "lr = 0.001\n",
        "train_dataloader = DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, pin_memory=True, batch_size=batch_size,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abp9wE--t3b1"
      },
      "source": [
        "# SegFormer Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-MvayrKhGFC"
      },
      "outputs": [],
      "source": [
        "classes = pd.read_csv(f'{root_dir}/class_dict_seg.csv')['name'] # remove /content/ when not in colab\n",
        "id2label = classes.to_dict()\n",
        "label2id = {v: k for k, v in id2label.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZYWZDMohKQD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n",
        "                                                         num_labels=len(id2label), id2label=id2label, label2id=label2id,\n",
        "                                                         reshape_last_stage=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB-uAxRVhR92"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.0001)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(torch.cuda.is_available())\n",
        "print(device)\n",
        "print(\"Model Initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk2jlYcoY12B"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "# import gc\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "mean_iou = evaluate.load(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnT-FE2RzPKw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_loss_values = []\n",
        "val_loss_values = []\n",
        "mean_iou_values = []\n",
        "train_acc_values = []\n",
        "val_acc_values = []\n",
        "train_rec_values = []\n",
        "val_rec_values = []\n",
        "for epoch in range(1, epochs):  # loop over the dataset multiple times\n",
        "    print(\"Epoch:\", epoch)\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    accuracies = []\n",
        "    recalls = []\n",
        "    losses = []\n",
        "    val_accuracies = []\n",
        "    val_recalls = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(pbar):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        # evaluate\n",
        "        upsampled_logits = nn.functional.interpolate(outputs.logits.to(device), size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(true_labels, pred_labels)\n",
        "        recall = recall_score(true_labels, pred_labels)\n",
        "        recalls.append(recall)\n",
        "        loss = outputs.loss\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Recall':sum(recalls)/len(recalls), 'Loss': sum(losses)/len(losses)})\n",
        "\n",
        "        # backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for idx, batch in enumerate(valid_dataloader):\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                labels = (batch[\"labels\"].to(device))\n",
        "\n",
        "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "                upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "                predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "                mask = (labels != 255)\n",
        "                pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "                true_labels = labels[mask].detach().cpu().numpy()\n",
        "                accuracy = accuracy_score(true_labels, pred_labels)\n",
        "                recall = recall_score(true_labels, pred_labels)\n",
        "                val_recalls.append(recall)\n",
        "                val_loss = outputs.loss\n",
        "                val_accuracies.append(accuracy)\n",
        "                val_losses.append(val_loss.item())\n",
        "                predictions = predicted.detach().cpu().numpy()\n",
        "                actual = labels.detach().cpu().numpy()\n",
        "                results_mean_iou = mean_iou.compute(predictions=predictions, references=actual, ignore_index=0, num_labels=2)\n",
        "\n",
        "\n",
        "    mean_iou_values.append(results_mean_iou['mean_iou'])\n",
        "    # Calculate and append the training loss\n",
        "    train_loss_values.append(sum(losses)/len(losses))\n",
        "    # Calculate and append the validation loss\n",
        "    val_loss_values.append(sum(val_losses)/len(val_losses))\n",
        "    # Calculate and append the training accuracy\n",
        "    train_acc_values.append(sum(accuracies)/len(accuracies))\n",
        "    # Calculate and append the validation accuracy\n",
        "    val_acc_values.append(sum(val_accuracies)/len(val_accuracies))\n",
        "    # Calculate and append the training recall\n",
        "    train_rec_values.append(sum(recalls)/len(recalls))\n",
        "    # Calculate and append the validation recall\n",
        "    val_rec_values.append(sum(val_recalls)/len(val_recalls))\n",
        "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
        "         Recall: {sum(recalls)/len(recalls)}\\\n",
        "         Train Loss: {sum(losses)/len(losses)}\\\n",
        "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
        "         Val Recall: {sum(val_recalls)/len(val_recalls)}\\\n",
        "         Val Loss: {sum(val_losses)/len(val_losses)}\\\n",
        "         IoU: {results_mean_iou}\")\n",
        "\n",
        "    # if epoch % 5 == 0:\n",
        "    #     torch.save({\n",
        "    #             'epoch': epoch,\n",
        "    #             'model_state_dict': model.state_dict(),\n",
        "    #             'optimizer_state_dict': optimizer.state_dict(),\n",
        "    #             'loss': sum(val_losses)/len(val_losses),\n",
        "    #             }, f'./checkpoint_{epoch}_lr:{lr}_bs:{batch_size}.pth')\n",
        "\n",
        "print(train_loss_values)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt_epochs = np.arange(1, epochs)\n",
        "plt.plot(plt_epochs, train_loss_values, label='Training Loss')\n",
        "plt.plot(plt_epochs, val_loss_values, label='Validation Loss')\n",
        "plt.xticks(plt_epochs)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Training and Validation Loss LR={lr} BS={batch_size}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL6H-xv2Lbrw"
      },
      "source": [
        "#Plot the mIoU of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_cDdBe3PpgM"
      },
      "outputs": [],
      "source": [
        "plt.plot(plt_epochs, mean_iou_values, label='Mean IoU')\n",
        "plt.title(f'Mean IoU LR={lr} BS={batch_size}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs0rRZ0aLiId"
      },
      "source": [
        "#Plot the training and validation accuracies of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu7xKsubLZsj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt_epochs = np.arange(1, epochs)\n",
        "plt.plot(plt_epochs, train_acc_values, label='Training Accuracy')\n",
        "plt.plot(plt_epochs, val_acc_values, label='Validation Accuracy')\n",
        "plt.xticks(plt_epochs)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'Training and Validation Accuracy LR={lr} BS={batch_size}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykLopQ3uLRJb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt_epochs = np.arange(1, epochs)\n",
        "plt.plot(plt_epochs, train_rec_values, label='Training Recall')\n",
        "plt.plot(plt_epochs, val_rec_values, label='Validation Recall')\n",
        "plt.xticks(plt_epochs)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.title(f'Training and Validation Recall LR={lr} BS={batch_size}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqqlXzkWhjoK"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"<GIVE SAVE PATH>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mizQD5EHGK04"
      },
      "source": [
        "#Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge29hsRihR7B"
      },
      "outputs": [],
      "source": [
        "directory = '<GIVE TEST SET PATH>'\n",
        "dir_size = 0\n",
        "for file in os.listdir(directory):\n",
        "    f = os.path.join(directory, file)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        dir_size += 1\n",
        "\n",
        "fig_width = 10  # Adjust this value for the desired width of each image\n",
        "fig_height = (dir_size * 512) / (2 * 512) * fig_width\n",
        "fig, axs = plt.subplots(dir_size, 2, figsize=(fig_width, fig_height))\n",
        "\n",
        "count = 0\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        image_name = filename\n",
        "        image = cv2.imread(f'{root_dir}/images/test/{image_name}')\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(f'{root_dir}/mask/test/{image_name}', 0)\n",
        "        axs[count][0].set_aspect('auto')  # Adjust the aspect ratio as needed\n",
        "        axs[count][0].axis('off')\n",
        "        axs[count][1].set_aspect('auto')  # Adjust the aspect ratio as needed\n",
        "        axs[count][1].axis('off')\n",
        "        axs[count][0].imshow(image)\n",
        "        axs[count][1].imshow(mask)\n",
        "        count += 1\n",
        "\n",
        "pan_sharp = \"./TrainingDataset_Pan\"\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzLYJxPUALzh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(f'{root_dir}/class_dict_seg.csv')\n",
        "classes = df['name']\n",
        "palette = df[[' r', ' g', ' b']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvlYgYlkEJz8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "feature_extractor_inference = SegformerFeatureExtractor(do_random_crop=False, do_pad=False)\n",
        "accuracy_values = []\n",
        "recall_values = []\n",
        "precision_values = []\n",
        "specificity_values = []\n",
        "mean_iou_values_inf = []\n",
        "count_inference = 0\n",
        "fig2, axs2 = plt.subplots(1, 3, figsize=(fig_width, 0))\n",
        "axs2[0].set_title(\"Test samples\")\n",
        "axs2[1].set_title(\"Ground truth\")\n",
        "axs2[2].set_title(\"SegFormer MiT-b5\")\n",
        "axs2[0].axis('off')\n",
        "axs2[1].axis('off')\n",
        "axs2[2].axis('off')\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        image_name = filename\n",
        "        image = cv2.imread(f'{root_dir}/images/test/{image_name}')\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask_inf = cv2.imread(f'{root_dir}/mask/test/{image_name}', 0)\n",
        "        mask_inf[mask_inf == 255] = 1\n",
        "        pixel_values = feature_extractor_inference(image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "        model.eval()\n",
        "        outputs = model(pixel_values=pixel_values)# logits are of shape (batch_size, num_labels, height/4, width/4)\n",
        "        logits = outputs.logits.cpu()\n",
        "        upsampled_logits = nn.functional.interpolate(logits,\n",
        "                size=image.shape[:-1], # (height, width)\n",
        "                mode='bilinear',\n",
        "                align_corners=False)\n",
        "        # Second, apply argmax on the class dimension\n",
        "        seg = upsampled_logits.argmax(dim=1)[0]\n",
        "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\\\n",
        "\n",
        "        mask_bool = (mask_inf != 255)\n",
        "        predicted = seg[mask_bool].detach().cpu().numpy()\n",
        "        real = mask_inf[mask_bool]\n",
        "        for label, color in enumerate(palette):\n",
        "            color_seg[seg == label, :] = color\n",
        "        # Convert to BGR\n",
        "        color_seg = color_seg[..., ::-1]\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(real, predicted).ravel()\n",
        "        accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
        "        recall = tp/(tp + fn)\n",
        "        precision = tp/(tp + fp)\n",
        "        specificity = tn/(tn + fp)\n",
        "        predicted_inf = [seg.detach().cpu().numpy().astype(np.int64)]\n",
        "        actual_inf = [mask_inf.astype(np.int64)]\n",
        "        results_mean_iou_inf = mean_iou.compute(predictions=predicted_inf, references=actual_inf, ignore_index=0, num_labels=2)\n",
        "\n",
        "        mean_iou_values_inf.append(results_mean_iou_inf['mean_iou'])\n",
        "        accuracy_values.append(accuracy)\n",
        "        recall_values.append(recall)\n",
        "        precision_values.append(precision)\n",
        "        specificity_values.append(specificity)\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(fig_width, fig_height))\n",
        "        # Show images witho ut the axes, with image name.\n",
        "        axs[0].set_aspect('auto')\n",
        "        axs[1].set_aspect('auto')\n",
        "        axs[2].set_aspect('auto')\n",
        "        axs[0].imshow(image)\n",
        "        axs[1].imshow(mask_inf)\n",
        "        axs[2].imshow(color_seg)\n",
        "        axs[0].axis('off')\n",
        "        axs[1].axis('off')\n",
        "        axs[0].set_title(filename[6:-4])\n",
        "        plt.setp(axs[2].get_xticklabels(), visible=False)\n",
        "        plt.setp(axs[2].get_yticklabels(), visible=False)\n",
        "        axs[2].tick_params(axis='both', which='both', length=0)\n",
        "        count_inference += 1\n",
        "        plt.show()\n",
        "print(\"Average pixel-wise accuracy: \", sum(accuracy_values)/len(accuracy_values))\n",
        "print(\"Average recall: \", sum(recall_values)/len(recall_values))\n",
        "print(\"Average precision: \", sum(precision_values)/len(precision_values))\n",
        "print(\"Average specificity: \", sum(specificity_values)/len(specificity_values))\n",
        "print(\"Mean IoU: \", sum(mean_iou_values_inf)/len(mean_iou_values_inf))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of parameters of the model\n"
      ],
      "metadata": {
        "id": "XtLlB9ddlhT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "print(pytorch_trainable_params)\n",
        "print(pytorch_total_params)"
      ],
      "metadata": {
        "id": "9M9INAMWlcCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}